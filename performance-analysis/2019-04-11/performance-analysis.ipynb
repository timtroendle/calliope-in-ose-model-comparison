{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring impact of tolerances on Gurobi\n",
    "\n",
    "Following the second analysis, I did some necessary changes to the model and the numerics look worse again:\n",
    "\n",
    "```\n",
    "[2019-04-11 14:03:10] SOLVER   Matrix range     [4e-06, 3e+02]\n",
    "[2019-04-11 14:03:10] SOLVER   Objective range  [1e+00, 1e+00]\n",
    "[2019-04-11 14:03:10] SOLVER   Bounds range     [5e-02, 2e+03]\n",
    "[2019-04-11 14:03:10] SOLVER   RHS range        [7e-03, 2e+03]\n",
    "```\n",
    "\n",
    "In the last round of performance runs the best parameter setting solved the model in less than 2 hours. Now, some scenarios aren't solved within 48h using the same parameter values. So I wanted to see whether I can find better values for the adapted model. Because I did not change anything related to the objective function, I kept `OptimalityTol` constant at 1e-5. I did vary `FeasibilityTol` for the baseline scenario and applied the following values:\n",
    "\n",
    "* 1e-6 (default value)\n",
    "* 1e-5\n",
    "* 1e-4\n",
    "* 1e-3\n",
    "* 1e-2 (max value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLVE_DURATION_PREFIX = \"SOLVER   Solved in\"\n",
    "SOLUTION_PREFIX = \"SOLVER   Optimal objective\"\n",
    "\n",
    "\n",
    "def _preprocess_log(path_to_log):\n",
    "    path_to_log = Path(path_to_log)\n",
    "    with path_to_log.open(\"r\") as log_file:\n",
    "        lines = log_file.readlines()\n",
    "    return [line[22:] for line in lines]\n",
    "    \n",
    "\n",
    "def parse_solution_time(path_to_log):\n",
    "    lines = _preprocess_log(path_to_log)\n",
    "    duration_line = list(filter(lambda line: line.startswith(SOLVE_DURATION_PREFIX), lines))[0]\n",
    "    return float(re.findall('\\d+.?\\d+', duration_line)[1])\n",
    "\n",
    "\n",
    "def parse_iterations(path_to_log):\n",
    "    lines = _preprocess_log(path_to_log)\n",
    "    iterations_line = list(filter(lambda line: line.startswith(SOLVE_DURATION_PREFIX), lines))[0]\n",
    "    return float(re.findall('\\d+.?\\d+', iterations_line)[0])\n",
    "\n",
    "\n",
    "def parse_solution(path_to_log):\n",
    "    lines = _preprocess_log(path_to_log)\n",
    "    solution_line = list(filter(lambda line: line.startswith(SOLUTION_PREFIX), lines))[0]\n",
    "    return float(re.findall('\\d+.?\\d+e?\\+?\\d+', solution_line)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_runs(path_to_root_folder):\n",
    "    durations = [parse_solution_time(path) for path in Path(path_to_root_folder).glob(\"*.log\")]\n",
    "    solutions = [parse_solution(path) for path in Path(path_to_root_folder).glob(\"*.log\")]\n",
    "    iterations = [parse_iterations(path) for path in Path(path_to_root_folder).glob(\"*.log\")]\n",
    "    \n",
    "    mean, std = stats.norm.fit(solutions)\n",
    "    print(f\"Gurobi found the following optimal value on average: {mean} (std {std}).\")\n",
    "    mean, std = stats.norm.fit(iterations)\n",
    "    print(f\"Gurobi needed on average {mean:.0f} (std {std:.0f}) iterations to find the optimal value.\")\n",
    "    mean, std = stats.norm.fit(durations)\n",
    "    print(f\"Gurobi needed on average {mean:.0f}s (std {std:.0f}s) to find the optimal value.\")\n",
    "\n",
    "    \n",
    "def diff(path_to_root_folder, path_to_root_folder_of_base):\n",
    "    durations = [parse_solution_time(path) for path in Path(path_to_root_folder).glob(\"*.log\")]\n",
    "    solutions = [parse_solution(path) for path in Path(path_to_root_folder).glob(\"*.log\")]\n",
    "    iterations = [parse_iterations(path) for path in Path(path_to_root_folder).glob(\"*.log\")]\n",
    "    \n",
    "    base_durations = [parse_solution_time(path) for path in Path(path_to_root_folder_of_base).glob(\"*.log\")]\n",
    "    base_solutions = [parse_solution(path) for path in Path(path_to_root_folder_of_base).glob(\"*.log\")]\n",
    "    base_iterations = [parse_iterations(path) for path in Path(path_to_root_folder_of_base).glob(\"*.log\")]\n",
    "    \n",
    "    mean, std = stats.norm.fit(solutions)\n",
    "    mean_base, std_base = stats.norm.fit(base_solutions)\n",
    "    print(f\"Relative diff of objective average to base: {mean / mean_base}\")\n",
    "    mean, std = stats.norm.fit(iterations)\n",
    "    mean_base, std_base = stats.norm.fit(base_iterations)\n",
    "    print(f\"Relative diff of iterations average to base: {mean / mean_base}\")\n",
    "    mean, std = stats.norm.fit(durations)\n",
    "    mean_base, std_base = stats.norm.fit(base_durations)\n",
    "    print(f\"Relative diff of durations average to base: {mean / mean_base}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolerances e-6\n",
      "All runs needed more than 24h and were killed by the cluster.\n",
      "\n",
      "Tolerances e-5\n",
      "--------------\n",
      "Gurobi found the following optimal value on average: 272895.576 (std 0.0).\n",
      "Gurobi needed on average 1047331 (std 0) iterations to find the optimal value.\n",
      "Gurobi needed on average 22026s (std 103s) to find the optimal value.\n",
      "\n",
      "Tolerances e-4\n",
      "All runs needed more than 24h and were killed by the cluster.\n",
      "\n",
      "Tolerances e-3\n",
      "--------------\n",
      "All runs needed more than 24h and were killed by the cluster.\n",
      "\n",
      "Tolerances e-2\n",
      "--------------\n",
      "All runs needed more than 24h and were killed by the cluster.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tolerances e-6\")\n",
    "print(\"All runs needed more than 24h and were killed by the cluster.\")\n",
    "print()\n",
    "print(\"Tolerances e-5\")\n",
    "print(\"--------------\")\n",
    "analyse_runs(\"minus5/\")\n",
    "print()\n",
    "print(\"Tolerances e-4\")\n",
    "print(\"--------------\")\n",
    "print(\"All runs needed more than 24h and were killed by the cluster.\")\n",
    "print()\n",
    "print(\"Tolerances e-3\")\n",
    "print(\"--------------\")\n",
    "print(\"All runs needed more than 24h and were killed by the cluster.\")\n",
    "print()\n",
    "print(\"Tolerances e-2\")\n",
    "print(\"--------------\")\n",
    "print(\"All runs needed more than 24h and were killed by the cluster.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Only runs with `FeasibilityTol` of 1e5 were able to solve the problem within 24h. Seems it is still the best parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
